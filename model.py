# -*- coding: utf-8 -*-
"""finel0_47 (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zlveZTke6IKJ-In4C6uBUBHN8RRtqN2R
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import confusion_matrix
import numpy as np
import pandas as pd
import re
from sklearn.metrics import accuracy_score
import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')


batch_size = 30
seq_size = 32
num_layer = 4
dropout_size = 0.1
learning_rate = 0.0001

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

train_df = pd.read_csv("trainEmotions.csv")
test_df = pd.read_csv("testEmotions.csv")

train_list = list(train_df.to_records(index=False))
test_list = list(test_df.to_records(index=False))

def write_to_file(data,key):
  with open(key+'.txt', 'w') as filehandle:
    for listitem in data[key]:
        filehandle.write('%s\n' % listitem)
        
def clean_data(content):
    content = re.sub("@\w+", "", re.sub("https?:\/\/t.co/[\w]+", "", content))
    return content.lower()

train_list = [(label, word_tokenize(clean_data(content))) for label, content in train_list]
test_list = [(label, word_tokenize(clean_data(content))) for label, content in test_list]

index2word = ["<PAD>", "<SOS>", "<EOS>"]

for ds in [train_list, test_list]:
    for label, content in ds:
        for token in content:
            if token not in index2word:
                index2word.append(token)

word2index = {token: i for i, token in enumerate(index2word)}

def mapping_labels(label):
    if label == "sadness":
        return 0
    elif label == "neutral":
        return 1
    else: #happiness
        return 2

def encoding_padding(content, length):
    sos = [word2index["<SOS>"]]
    eos = [word2index["<EOS>"]]
    pad = [word2index["<PAD>"]]

    if len(content) < length - 2:
        n_pads = length - 2 - len(content)
        encoded = [word2index[i] for i in content]
        return sos + encoded + eos + pad * n_pads 
    else:
        encoded = [word2index[i] for i in content]
        return sos + encoded[:length - 2] + eos

encoded_train = [(encoding_padding(content, seq_size), mapping_labels(label)) for label, content in train_list]
encoded_test = [(encoding_padding(content, seq_size), mapping_labels(label)) for label, content in test_list]

data = {}
data['epoch_loss_train'] = []
data['epoch_loss_test'] = []
data['all_outputs_train'] = []
data['all_outputs_test'] = []
data['all_labels_train'] = []
data['all_labels_test'] = []

train_x = np.array([content for content, label in encoded_train])
train_y = np.array([label for content, label in encoded_train])
test_x = np.array([content for content, label in encoded_test])
test_y = np.array([label for content, label in encoded_test])

train_ds = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))
test_ds = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))


train_dataloader = DataLoader(train_ds, shuffle=True, batch_size=batch_size, drop_last=True)
test_dataloader = DataLoader(test_ds, shuffle=True, batch_size=batch_size, drop_last=True)

class model1(torch.nn.Module) :
    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout) :
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,num_layers= num_layer,dropout = dropout)
        self.dropout = nn.Dropout(dropout)
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 3)
        )

    def forward(self, x, hidden):
        embs = self.embedding(x)
        out, hidden = self.lstm(embs, hidden)
        out = self.dropout(out)
        out = self.fc(out)
        out = out[:, -1]
        return out, hidden
    
    def init_hidden(self):
        return (torch.zeros(num_layer, batch_size, 64), torch.zeros(num_layer, batch_size, 64))

model = model1(len(word2index), 64, 64, dropout_size)
model = model.to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)

epochs = 50
losses = []
data['accurcy_eph_train'] = []
data['accurcy_eph_test'] = []

for e in range(epochs):
    h0, c0 =  model.init_hidden()
    h0 = h0.to(device)
    c0 = c0.to(device)
    batch_acc_train = []
    data['all_outputs_train'] = []
    data['all_labels_train'] = []
    data['all_outputs_test'] = []
    data['all_labels_test'] = []
    losses_batch_train= []
    for batch_idx, batch in enumerate(train_dataloader):
        input = batch[0].to(device)
        target = batch[1].to(device)
        optimizer.zero_grad()
        with torch.set_grad_enabled(True):
            out, hidden = model(input, (h0, c0))
            _, preds = torch.max(out, 1)
            loss = criterion(out, target)
            data['all_outputs_train'].extend(preds.cpu().detach().numpy())
            data['all_labels_train'].extend(target.cpu().detach().numpy())
            losses_batch_train.append(loss)
            
            loss.backward()
            optimizer.step()
    data['accurcy_eph_train'].append(accuracy_score(data['all_outputs_train'],data['all_labels_train']))
    data['epoch_loss_train'].append((sum(losses_batch_train) / len(losses_batch_train)).cpu().detach().numpy())
    losses.append(loss.item())

    losses_batch_test= []
    for batch_idx, batch in enumerate(test_dataloader):
        batch_acc_test = []
        input = batch[0].to(device)
        target = batch[1].to(device)

        optimizer.zero_grad()
        with torch.set_grad_enabled(False):
            out, hidden = model(input, (h0, c0))
            _, preds = torch.max(out, 1)
            preds = preds.to("cpu").tolist()
            data['all_outputs_test'].extend(preds)
            loss = criterion(out, target)
            losses_batch_test.append(loss)
            data['all_labels_test'].extend(target.cpu().detach().numpy())
    data['accurcy_eph_test'].append(accuracy_score(data['all_outputs_test'],data['all_labels_test']))
    data['epoch_loss_test'].append((sum(losses_batch_test)/len(losses_batch_test)).cpu().detach().numpy())

confusion_matrix(data['all_labels_test'], data['all_outputs_test'])

#loss_train = []
#for i in data['epoch_loss_train']:
#    loss_train.append(float(i))
#loss_test = []
#for i in data['epoch_loss_test']:
#    loss_test.append(float(i))

#plt.plot(loss_train, label = 'epoch loss  - train')
#plt.plot(loss_test, label = 'epoch loss  - test')
#plt.title('loss on train and test sets')
#plt.legend()
#plt.xlabel('epochs')
#plt.ylabel('loss')
#plt.show()

#acc_train = []
#for i in accurcy_eph_train:
#    acc_train.append(float(i))
#acc_test = []
#for i in accurcy_eph_test:
#    acc_test.append(float(i))

#plt.plot(acc_train, label = 'epoch accurcy  - train')
#plt.plot(acc_test, label = 'epoch accurcy  - test')
#plt.title('accurcy on train and test sets')
#plt.legend()
#plt.xlabel('epochs')
#plt.ylabel('accurcy')
#plt.show()



torch.save(model, 'model.pkl')