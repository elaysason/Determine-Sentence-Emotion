# -*- coding: utf-8 -*-
"""predict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1iiAPEguKGFq4atrKdI9FPjQZCOV59KLw
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

from sklearn.metrics import confusion_matrix

import numpy as np
import pandas as pd

import re

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

import nltk
from nltk.tokenize import word_tokenize
nltk.download('punkt')
torch.manual_seed(42)
import os
import argparse
from torchvision import transforms
from torch.utils.data.dataset import Dataset
from PIL import Image
import torch.nn as nn
from skimage import io
from sklearn.metrics import accuracy_score

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

parser = argparse.ArgumentParser(description='Process input')
parser.add_argument('input_file', type=str, help='Input folder path, containing images')
args = parser.parse_args()

test_df = pd.read_csv(args.input_file)


test_set = list(test_df.to_records(index=False))

def remove_links_mentions(tweet):
    link_re_pattern = "https?:\/\/t.co/[\w]+"
    mention_re_pattern = "@\w+"
    tweet = re.sub(link_re_pattern, "", tweet)
    tweet = re.sub(mention_re_pattern, "", tweet)
    return tweet.lower()


test_set = [(label, word_tokenize(remove_links_mentions(tweet))) for label, tweet in test_set]

index2word = ["<PAD>", "<SOS>", "<EOS>"]

for ds in [test_set]:
    for label, tweet in ds:
        for token in tweet:
            if token not in index2word:
                index2word.append(token)

word2index = {token: idx for idx, token in enumerate(index2word)}

def label_map(label):
    if label == "sadness":
        return 0
    elif label == "neutral":
        return 1
    else: #happyness
        return 2

def encode_and_pad(tweet, length):
    sos = [word2index["<SOS>"]]
    eos = [word2index["<EOS>"]]
    pad = [word2index["<PAD>"]]

    if len(tweet) < length - 2: # -2 for SOS and EOS
        n_pads = length - 2 - len(tweet)
        encoded = [word2index[w] for w in tweet]
        return sos + encoded + eos + pad * n_pads 
    else: # tweet is longer than possible; truncating
        encoded = [word2index[w] for w in tweet]
        truncated = encoded[:length - 2]
        return sos + truncated + eos

seq_length = 32
layers_number = 4
dropout_num = 0.1
test_encoded = [(encode_and_pad(tweet, seq_length), label_map(label)) for label, tweet in test_set]

batch_size = 30
data = {}
data['epoch_loss_train'] = []
data['epoch_loss_test'] = []
data['all_outputs_train'] = []
data['all_outputs_test'] = []
data['all_labels_train'] = []
data['all_labels_test'] = []


test_x = np.array([tweet for tweet, label in test_encoded])
test_y = np.array([label for tweet, label in test_encoded])


test_ds = TensorDataset(torch.from_numpy(test_x).to(device), torch.from_numpy(test_y).to(device))
test_dl = DataLoader(test_ds, shuffle=True, batch_size=batch_size, drop_last=True)

class model1(torch.nn.Module) :
    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout) :
        super().__init__()

        # The embedding layer takes the vocab size and the embeddings size as input
        # The embeddings size is up to you to decide, but common sizes are between 50 and 100.
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)

        # The LSTM layer takes in the the embedding size and the hidden vector size.
        # The hidden dimension is up to you to decide, but common values are 32, 64, 128
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,num_layers= layers_number,dropout = dropout_num)

        # We use dropout before the final layer to improve with regularization
        self.dropout = nn.Dropout(dropout_num)

        # The fully-connected layer takes in the hidden dim of the LSTM and
        #  outputs a a 3x1 vector of the class scores.
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim, 16),
            nn.ReLU(),
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Linear(8, 3))

    def forward(self, x, hidden):
        """
        The forward method takes in the input and the previous hidden state 
        """

        # The input is transformed to embeddings by passing it to the embedding layer
        embs = self.embedding(x)

        # The embedded inputs are fed to the LSTM alongside the previous hidden state
        out, hidden = self.lstm(embs, hidden)

        # Dropout is applied to the output and fed to the FC layer
        out = self.dropout(out)
        out = self.fc(out)

        # We extract the scores for the final hidden state since it is the one that matters.
        out = out[:, -1]
        return out, hidden
    
    def init_hidden(self):
        return (torch.zeros(layers_number, batch_size, 64), torch.zeros(layers_number, batch_size, 64))





model = torch.load('model.pkl').to(device)#######

data = {'prediction':[],'content':[]}
dataeval = {}

len(word2index)



def unmap_label(label):
    if label == 0:
        return 'sadness'
    elif label == 1:
        return 'neutral'
    else: #2
        return 'happiness'

h0, c0 =  model.init_hidden()
h0 = h0.to(device)
c0 = c0.to(device)
batch_acc_test = []
expected = []
for batch_idx, batch in enumerate(test_dl):
    input = batch[0].to(device)
    target = batch[1].to(device)
    
    expected.extend([unmap_label(i) for i in target])
    
    
    with torch.set_grad_enabled(False):
        out, hidden = model(input, (h0, c0))
        _, preds = torch.max(out, 1)#################
        preds = preds.to(device).tolist()
        data['prediction'].extend([unmap_label(i) for i in preds])
        data['content'].extend(test_df['content'][batch_idx*batch_size:batch_idx*batch_size+batch_size])
        
        batch_acc_test.append(accuracy_score(preds, target.tolist()))


predicted_df = pd.DataFrame(data)
predicted_df.to_csv("prediction.csv", index=False, header=True)